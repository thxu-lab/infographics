<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ecosistema de Modelos de Lenguaje</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Poppins:wght@400;600;700&display=swap');

        :root {
            --bg-color: #f4f7f9;
            --card-bg-color: #ffffff;
            --primary-text-color: #2c3e50;
            --secondary-text-color: #576574;
            --accent-color: #3498db;
            --border-color: #e1e5e8;
            --shadow-color: rgba(0, 0, 0, 0.08);
        }

        body {
            font-family: 'Poppins', sans-serif;
            background-color: var(--bg-color);
            color: var(--primary-text-color);
            margin: 0;
            padding: 20px;
            line-height: 1.7;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: var(--card-bg-color);
            border-radius: 12px;
            box-shadow: 0 8px 32px var(--shadow-color);
            overflow: hidden;
        }

        header {
            background-color: var(--accent-color);
            color: white;
            padding: 30px 40px;
            text-align: center;
        }

        header h1 {
            margin: 0;
            font-size: 2.5em;
            font-weight: 700;
        }

        header .intro {
            margin: 10px 0 0;
            font-size: 1.1em;
            opacity: 0.9;
        }

        main {
            padding: 20px 40px;
        }

        section {
            margin-bottom: 40px;
            padding-bottom: 30px;
            border-bottom: 2px solid var(--border-color);
        }

        section:last-child {
            border-bottom: none;
            margin-bottom: 0;
        }

        h2 {
            font-size: 2em;
            color: var(--accent-color);
            margin-top: 0;
            margin-bottom: 20px;
            font-weight: 600;
            display: flex;
            align-items: center;
        }
        
        h2 .icon {
            margin-right: 15px;
            font-size: 1.2em;
        }

        h3 {
            font-size: 1.3em;
            color: var(--primary-text-color);
            margin-top: 25px;
            margin-bottom: 10px;
            border-left: 4px solid var(--accent-color);
            padding-left: 10px;
            font-weight: 600;
        }

        p, ul, ol {
            color: var(--secondary-text-color);
            font-size: 1em;
        }

        ul, ol {
            padding-left: 25px;
        }
        
        li {
            margin-bottom: 10px;
        }

        strong, b {
            color: var(--primary-text-color);
            font-weight: 600;
        }

        code {
            background-color: #e8f4fd;
            color: var(--accent-color);
            padding: 3px 6px;
            border-radius: 4px;
            font-family: 'Courier New', Courier, monospace;
            font-weight: 600;
        }

        .glossary dl {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 20px;
        }

        .glossary dt {
            font-weight: 600;
            color: var(--primary-text-color);
        }

        .glossary dd {
            margin-left: 0;
            color: var(--secondary-text-color);
            padding-bottom: 10px;
            border-bottom: 1px dashed var(--border-color);
        }

        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            main {
                padding: 15px 25px;
            }
            header {
                padding: 25px;
            }
            h1 {
                font-size: 2em;
            }
        }
    </style>
</head>
<body>

    <div class="container">
        <header>
            <h1>Ecosistema de Modelos de Lenguaje</h1>
            <p class="intro">Una gu√≠a para entender los conceptos fundamentales que hacen funcionar a los modelos de lenguaje modernos.</p>
        </header>
        
        <main>
            <section id="embeddings">
                <h2><span class="icon">üß¨</span>Embeddings: Representando el Significado</h2>
                <p>Un <strong>embedding</strong> es una representaci√≥n vectorial (una lista de n√∫meros) que captura el significado sem√°ntico de datos como palabras, frases o im√°genes. Esto permite que las m√°quinas entiendan las relaciones y similitudes entre conceptos.</p>
                
                <h3>Tipos de Embeddings</h3>
                <ul>
                    <li><strong>Est√°ticos:</strong> Vectores fijos y precalculados como <code>Word2Vec</code> o <code>GloVe</code>. No cambian seg√∫n el contexto de la frase.</li>
                    <li><strong>Contextuales:</strong> Vectores din√°micos generados por modelos como <code>BERT</code> o <code>GPT</code>. Var√≠an seg√∫n las palabras que los rodean para capturar matices.</li>
                </ul>

                <h3>Propiedades Clave</h3>
                <ul>
                    <li><strong>Dimensionalidad:</strong> El tama√±o del vector (p. ej., 300, 768). Un mayor n√∫mero puede capturar m√°s informaci√≥n, pero a un mayor costo computacional.</li>
                    <li><strong>M√©tricas de Similitud:</strong> Se usan operaciones matem√°ticas como la üìê <strong>Similitud del Coseno</strong> o la üìè <strong>Distancia Euclidiana</strong> para medir qu√© tan parecidos son dos vectores.</li>
                </ul>

                <h3>Casos de Uso Principales</h3>
                <ul>
                    <li><strong>B√∫squeda sem√°ntica:</strong> Encontrar resultados que significan lo mismo, no solo que contienen las mismas palabras clave.</li>
                    <li><strong>Sistemas de recomendaci√≥n:</strong> Sugerir productos o contenidos similares.</li>
                    <li><strong>Aumento de contexto en LLMs (RAG):</strong> Encontrar informaci√≥n relevante para que el modelo la use en sus respuestas.</li>
                    <li><strong>Almacenamiento:</strong> Se guardan y consultan eficientemente en <strong>Bases de Datos Vectoriales</strong> (p. ej. <code>Pinecone</code>, <code>Milvus</code>) usando algoritmos <strong>ANN</strong>.</li>
                </ul>
            </section>
            
            <section id="fine-tuning">
                <h2><span class="icon">üõ†Ô∏è</span>Fine-Tuning: Especializando el Modelo</h2>
                <p>El <strong>fine-tuning</strong> es el proceso de re-entrenar un modelo preexistente con un conjunto de datos m√°s peque√±o y espec√≠fico para adaptarlo a una tarea o dominio particular.</p>

                <h3>Enfoques Principales</h3>
                <ul>
                    <li><strong>Full Fine-Tuning:</strong> Se actualizan <em>todos</em> los par√°metros del modelo. M√°xima adaptabilidad, pero requiere muchos recursos computacionales.</li>
                    <li><strong>PEFT (Parameter-Efficient Fine-Tuning):</strong> Solo se ajusta un peque√±o subconjunto de par√°metros (p. ej., <code>LoRA</code>). Mucho m√°s eficiente en costo y almacenamiento.</li>
                </ul>
                
                <h3>Proceso de Fine-Tuning</h3>
                <ol>
                    <li><strong>Selecci√≥n de Datos:</strong> Recopilar un dataset limpio y representativo de la tarea.</li>
                    <li><strong>Configuraci√≥n:</strong> Definir hiperpar√°metros como la tasa de aprendizaje y el n√∫mero de √©pocas.</li>
                    <li><strong>Entrenamiento:</strong> Ajustar los pesos del modelo para minimizar el error.</li>
                    <li><strong>Evaluaci√≥n:</strong> Medir el rendimiento con m√©tricas como precisi√≥n o F1.</li>
                    <li><strong>Despliegue:</strong> Integrar el modelo afinado en un sistema de producci√≥n.</li>
                </ol>

                <h3>‚ö†Ô∏è Consideraciones Pr√°cticas</h3>
                <p>Hay que vigilar el <strong>Overfitting</strong> (el modelo memoriza los datos pero no generaliza) y el <strong>Catastrophic Forgetting</strong> (el modelo olvida su conocimiento general).</p>
            </section>
            
            <section id="tokenizacion">
                <h2><span class="icon">üß©</span>Tokens y Tokenizaci√≥n: Las Piezas del Lenguaje</h2>
                <p>La <strong>tokenizaci√≥n</strong> es el proceso de dividir el texto en las unidades m√≠nimas que un modelo puede procesar, llamadas <strong>tokens</strong>. Un token puede ser una palabra, parte de una palabra o un caracter.</p>
                
                <h3>Algoritmos Comunes</h3>
                <ul>
                    <li><strong>Word-Level:</strong> Cada palabra es un token. Genera vocabularios muy grandes.</li>
                    <li><strong>Subword (BPE, WordPiece):</strong> El m√°s com√∫n. Divide las palabras en sub-unidades frecuentes. Balancea el tama√±o del vocabulario y maneja palabras desconocidas.</li>
                    <li><strong>Character-Level:</strong> Cada caracter es un token. Vocabulario muy peque√±o, pero secuencias muy largas.</li>
                </ul>
                
                <h3>Impacto en el Modelo</h3>
                <p>La tokenizaci√≥n afecta la <strong>longitud m√°xima de la secuencia</strong> que el modelo puede procesar, el <strong>costo computacional</strong> (m√°s tokens = m√°s c√≥mputo) y la <strong>calidad de la representaci√≥n</strong>.</p>
            </section>
            
            <section id="inferencia-despliegue">
                <h2><span class="icon">üöÄ</span>Inferencia y Despliegue: Del Laboratorio a la Realidad</h2>
                <p>La <strong>inferencia</strong> es el uso de un modelo ya entrenado para hacer predicciones. El <strong>despliegue</strong> es el proceso de hacer que ese modelo est√© disponible para los usuarios en un entorno real.</p>
                
                <h3>T√©cnicas de Optimizaci√≥n</h3>
                <ul>
                    <li><strong>Quantization:</strong> Reducir la precisi√≥n de los n√∫meros del modelo (p. ej., de 32 a 8 bits) para acelerar los c√°lculos.</li>
                    <li><strong>Pruning:</strong> Eliminar conexiones neuronales irrelevantes del modelo para hacerlo m√°s ligero.</li>
                    <li><strong>Knowledge Distillation:</strong> Entrenar un modelo peque√±o ("estudiante") para que imite a uno grande ("profesor").</li>
                </ul>

                <h3>Infraestructura Clave</h3>
                <p>El despliegue moderno se apoya en <strong>Contenedores</strong> (<code>Docker</code>, <code>Kubernetes</code>) para escalar, <strong>Servidores de Modelos</strong> (<code>Triton</code>, <code>TorchServe</code>) para entregar predicciones eficientemente, y plataformas para <strong>Edge Deployment</strong> (<code>TensorFlow Lite</code>) para ejecutar modelos en dispositivos locales.</p>
            </section>
            
            <section id="glosario" class="glossary">
                <h2><span class="icon">üìö</span>Glosario R√°pido: T√©rminos Esenciales</h2>
                <dl>
                    <dt>Adam</dt>
                    <dd>Optimizador que combina Momentum y RMSProp. Es el m√°s utilizado.</dd>
                    
                    <dt>Dropout</dt>
                    <dd>T√©cnica de regularizaci√≥n que "apaga" neuronas al azar para evitar el overfitting.</dd>
                    
                    <dt>Epoch</dt>
                    <dd>Una pasada completa del algoritmo de entrenamiento sobre todo el conjunto de datos.</dd>

                    <dt>Kubernetes</dt>
                    <dd>Sistema de orquestaci√≥n de contenedores para desplegar y escalar aplicaciones.</dd>
                    
                    <dt>Learning Rate</dt>
                    <dd>Par√°metro que controla cu√°nto se ajustan los pesos del modelo durante el entrenamiento.</dd>

                    <dt>Pruning</dt>
                    <dd>Eliminaci√≥n de pesos menos relevantes para simplificar el modelo y acelerar la inferencia.</dd>

                    <dt>Quantization</dt>
                    <dd>T√©cnica para reducir la precisi√≥n num√©rica de los pesos, acelerando la inferencia.</dd>
                    
                    <dt>SGD</dt>
                    <dd>Stochastic Gradient Descent; algoritmo de optimizaci√≥n fundamental.</dd>

                    <dt>Weight Decay</dt>
                    <dd>T√©cnica de regularizaci√≥n (L2) que penaliza los pesos grandes para prevenir el overfitting.</dd>
                </dl>
            </section>
        </main>
    </div>

</body>
</html>
